---
layout: article
title:  "「CV」 目标检测概述"
date:   2020-06-04 16:00:40 +0800
key: ObjectDetection-survey
aside:
  toc: true
category: [AI, CV, detection]
---
<span id='head'> </span>
[目标检测资源](/ai/cv/detection/2019/05/10/foundation.html)     

<center class="half">
  <img src="/assets/images/cv/detection/overview.png" /><br>图1：目标检测发展路线&emsp;
</center>

<!--more-->  

# 1 技术点

# 2 经典网络
## 2.1 anchor
**1. two stage**  

<center class="half">
  <img src="/assets/images/cv/detection/two_stage.png" /><br>图2：两阶段算法演进路线&emsp;
</center>

**2. one stage**   

| 模型     | 目标 | backbone | neck | head | 缺点 |
| ---     | --- | --- | --- | --- | --- |
| [SSD](#SSD)   | 正负样本不均衡<br>速度和精度的平衡 | VGG16 | 原始特征图 | 多层 | 小物体效果差<br>anchor要要手动设定<br>精度比两阶段低|
| [DSSD](#DSSD) | 小目标检测 | ResNet | 反卷积+高-低相乘| 多层 + 残差 | 巨慢 |
| [RSSD](#RSSD) | 小目标检测<br>误检| ResNet | 上下同时融合 + concate | 多层 <br> 分类共享 | 慢 |
| [RefineDet](#RefineDet) | 精度 | ResNet | FPN | 多层<br>RPN |  |
| [RFBNet](#RFBNet) | 精度 | VGG |  | 多层<br>RFB |  |
| [YOLOv1](#YOLOv1) | 速度 | darknet24 | - | 单层 | 分类、定位精度低<br>拥挤情况效果差 |
| [YOLOv2](#YOLOv2) | 精度 | darknet19 + anchor | pooling + concate | 单层 | 同上 |
| [YOLOv3](#YOLOv3) | 精度 | darknet53 + anchor | uppooling + concate | 多层<br>sigmoid | 同上、慢 |
| [YOLOv4](#YOLOv4) | 精度 | - | - | - | - |
| [RetinaNet](#RetinaNet) | 样本不均衡 | ResNet | FPN | 多层预测<br>sigmoid |  |
| M2Det | 多尺度目标 | VGG | MLFPN | 多层 | 慢 |

## 2.2 anchor free

| 模型 | 意义 | 创新点 |
| --- | --- | --- |
| RelationNet | 物体间的关系 | 提出目标关系模块 |
| DCNv2 | 几何形变问题 | 提出可变形卷积网络 DCN |
| NAS-FPN | 精度 | 自动搜索出的 FPN 结构 |

# 3 难点
## 3.1 常规组件
### 3.1.1 骨干网络
ResNeXT、CSPDarknet、CSPResNeXt、EfficientNet、MobileNet、ShuffleNet、GhostNet；    

### 3.1.2 Neck
SPP：Spatial pyramid pooling in deep convolutional networks for visual recognition    
ASPP：Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs    
RFB：Receptive field block net for accurate and fast object detection.   
SAM：Convolutional block attention module.   
FPN：Feature pyramid networks for object detection.   
PANet：Path aggregation network for instance segmentation.   
Bi-FPN：Scalable and efficient object detection   
ASFF：Learning spatial fusion for single-shot object detection  
SFAM：A single-shot object detector based on multi-level feature pyramid network（M2det）    
### 3.1.3 激活函数
LReLU、PReLU、ReLU6、Scaled Exponential Linear Unit (SELU)、Swish、hard-Swish、Mish、ROI操作、RoIAlign；    
### 3.1.4 损失函数
Focal loss、IOU Loss、GIOU Loss、CIOU Loss、DIOU Loss、KL-Loss、Smooth L1 Loss、Balanced L1 Loss、GHM；     
### 3.1.5 后处理
soft NMS, Softer NMS, DIoU NMS;   


## 3.2 精度
### 3.2.1 样本不均衡

### 3.2.2 过拟合

### 3.2.3 多尺度

### 3.2.4 遮挡&拥挤
1. repulsion loss:让proposal尽量远离和他overlap第二大的GT，同时也让不同GT上的proposals之间相互远离；    
1. softnms来适当的解决nms阈值原因造成的问题；    
1. 改进的nms；     
1. 分part来做，其实这个比较适合人体的，并不通用；     

## 3.3 性能
### 3.3.1 [轻量化模型](/ai/dl/cnn/2020/06/27/light-cnn.html)
### 3.3.2 模型压缩


# 4 优化方向

| 方向 | 策略 |
| --- | --- |
| backbone |  |
| 优化策略 | UnitBox，IoUNet |
| 损失函数 | L1,L2,focal loss |
| NMS | soft-nms，softer-nms，relation net，ConvNMS，NMSNet，YesNet |
| anchor 生成 | 滑窗，RPN，CornerNet，meta-anchor |
| 零样本 |  |


# 5 发展
小目标的检测(如小于30像素的目标物体)、遮挡面积较大的目标以及区分图像中与目标物体外形相似的非目标物体等问题;   
实时性:对于自动驾驶或汽车辅助驾驶等场景对实时处理能力要求较高；   
小数据量：微调的精度尚待提高；    
特定行业很难获取大量的监督数据；    

-------------------  
[End](#head)
{:.warning}  


# 附录
## A 经典网络
### a 两阶段

<span id='RCNN'>  </span>

**RCNN**
{:.warning}
首次把深度技术引入到检测中：将检测当分类问题处理；       
```
区域提取：选择性搜索
分类：先裁剪得到区域图，然后rensize，送入 CNN 提取特征，送给 SVM 及边框回归
```  
**缺点**：    
- 流程繁琐，非端到端的训练；    
- 精度低：resize 后信息信息损失；`所有CNN的输入不都会 resize 吗`{:.warning}     


<span id='SPPNet'>  </span>

**SPPNet**
{:.warning}
提升速度：相同位置重复提取特征问题；       
提升精度：解决 resize 问题；
```
区域提取：选择性搜索
分类：先 CNN 提取特征，再使用 SPP 裁剪得到区域特征图，然后送给 SVM 及边框回归
```  
**SPP**：空间金字塔池化；
一块区域，被池化成不同大小（比如4×4, 2×2, 1×1），然后 flatten，拼接在一起；`感觉 SPP 主要解决的问题并不是 resize 带来的精度损失，因为 pooling 也有损失的；最大的作用还是完成了动态池化`{:.warning}    
怎么做的池化：    

<span id='FastRCNN'>  </span>

**FastRCNN**
{:.warning}
端到端训练：分类用fc+softmax；用多任务损失；       
```
区域提取：选择性搜索
backbone：VGG
分类：同 SPPNet（用 ROIPooling 取代 SPP）
```  
**ROIPooling**：只做一次 Pooling；用最近邻简化了池化；计算步长时，直接向下取整，造成了一定的位置偏移；`两次精度损失在哪`{:.warning}    
**缺点**：区域提取时间太长 2-3s；     


<span id='FasterRCNN'>  </span>

**FasterRCNN**
{:.warning}
提速：区域提取用 anchor；          
```
区域提取：RPN
backbone：VGG
分类：在 RoIPooling 之前先做一次对 anchor 的过滤；其他同 FastRCNN；
```  

**anchor**：一些位置、大小固定的先验框，通常是几个框的中心点重合在一起；anchor 的设计将区域生成和 CNN 联系到了一起，降低了收敛的难度；        
**RPN**：     
- 使用二分类（前背景）和 box 回归，对 anchor 做初步筛选和位置调整；   
- 具体的，feature map 上一个点对应一组 anchor（9个），然后用 1×1 进行分类和回归`1×1用的好`{:.warning}；    
- 使用 IoU 进行排序，然后 NMS 后选出前 2000 个；   
- 训练的时候，最终选出来 256 个，其中正负 1：1，多出来的负样本随机抛弃；测试选 300 个；    
- 超出图像的部分直接被修剪；   

**SmoothL1**：最初选用的是二阶损失，但是误差大时，梯度过大，易发散；所以大于 1 的部分改成了线性；   
**优点**：高精度，尤其是小物体；   
**缺点**：    
- RPN 看不到全图，所以背景容易误检`瞎说，后续几乎所有方法都延续了 anchor 策略，也有误检低的`{:.warning}     
- NMS 会过滤遮挡的情况，需要改进；  
- RoIPooling 之后的 fc 占据了绝大多数参数，需要改进；    
- 速度还是慢；    

**issue**:    
1. 高 IoU 时，为啥会导致定位不准？    


<span id='HyperNet'>  </span>

**HyperNet**
{:.warning}
融入了 FPN；          
```
区域提取：RPN
backbone：FasterRCNN
```  
**改进**：    
- 先做 RoIPooling，然后进行 RPN（过滤负样本），再进行 NMS；`这直接导致了速度上升吧，RPN 对相同像素重复提取了特征`{:.warning}   
- backbone 最后进行了特征融合：低层下采样，上层反卷积，都和当前层进行 concate；     

小物体检测有提升，mAP 提高了 3%；    
各层特征融合前，需要做 LRN（BN 也可以吧），否则 norm 大的 feature 会压制 norm 小的 feature；    
上采样用双线性插值，反卷积：即转置卷积，先补0，旋转核，再卷积；    


<span id='RFCN'>  </span>

**RFCN**
{:.warning}
提速：RoIPooling 后移；          
```
backbone：FasterRCNN + ResNet101
```
把 RoIPooling 后的 fc 前移动（或者说去掉了），检测效果差，因为卷积具有平移不变性，不利于定位`瞎说的吧，平移不变是对不同的图说的吧，要不然为啥到处用 FPN 融合位置信息呢`{:.warning}，于是设计了位置敏感得分地图；    
还采用了空洞卷积；`确定整个精度的提升不是因为堆叠了复杂的结构`{:.warning}      
速度依然很慢；    


<span id='MaskRCNN'>  </span>

**MaskRCNN**
{:.warning}
多任务：加入了分割；          
提升精度：ROIAlign 代替 RoIPooling；    
```
backbone： ResNet101
neck： FPN
head： FasterRCNN
```
**RoIAlign**：池化步长直接用小数，不用取整；每个池化小窗内，划分四个格子（小数的），格子内用双线性插值，然后四个格子再用 MaxPooling；以此解决了 RoIPooling 中的精度损失；   
**FPN**：多层输出，一个用于分类回归，一个用于 mask 生成`是吗`{:.warning}      
**Mask** 分支：和最后的分类、回归同级别；每个类别预测一个 mask；`哎吗，这不慢吗`{:.warning}      
**anchor**：根据大小，按公式映射到不同的层；    


<span id='CascadeRCNN'>  </span>

**CascadeRCNN**
{:.warning}
提升精度：级联 FasterRCNN；    
```
backbone： FasterRCNN
```
**动机**：由于 IoU 阈值的大小对检测精度有着重要影响；尤其是候选框在 IoU 阈值附近时，检测效果比其他 IoU 好很多；`为啥呢`{:.warning}；    
**级联策略**：两阶段中的第二阶段重复三次，每一次 RoIPooling 的输入是上一次最终分类的结果；每一次的 IoU 阈值逐渐提高，以此解决单个阈值决定精度的局限性；   

### b 单阶段
<span id='SSD'>  </span>

**SSD**
{:.warning}
```
backbone： VGG16(conv4-3 之后的去掉了)   
neck： 单层特征图     
head： 多层预测；添加了c8（10×10）、c9（5×5）、c10（3×3）、c11（1×1） 联合 conv4-3（19×19） 一起做预测；    
添加的几层是先用 1×1 降通道，然后再降分辨率；     
损失函数： softmax-交叉熵，SmoothL1   
prior box：    
```
**triker：**    
1. 解决正负样本不均衡    
通过筛选控制正负比例在 1：3；    
前向传播完成后，将预测框和真值框按一定策略匹配（参见 issue 1），这样得到了正负样本；之后选所有正样本（通常<100），并按置信度对负样本降序排序，筛选出靠前的3倍于正样本数量的负样本；然后再对筛选所得的框进行反向传播；*反向传播时只取正样本的偏移量，负样本的舍弃；*    
1. 丰富的数据增强；   
颜色抖动，crop；     
1. 所有类别共享一个回归值，FasterRCNN 是不同类别用不同回归值；    
FasterRCNN 是从 FastRCNN 继承得来的分类策略，最终的 box 回归时，为每一个类别都回归了一个 box；而此处是直接 n 分类 + 1 个 box 回归值；       

**缺点：**    
1. 小物体检测不好，低层特征虽然有利于定位，但是语义信息不够，所以分类不够；即没有考虑特征图之间的关联；   
1. 先验框尺寸需要手动设定，要根据不同任务去修改`怎么改，聚类吗`{:.warning}     
1. 一阶段算法的精度还是比两阶段的低；   
1. 多个特征图在检测同一物体，NMS 后仍然不能去除误检；`难道同一层的 Anchor 就不会检测到同一物体了，归罪于多层独立是不是不合理`{:.warning}     

**issue：**     
1. default box 与 真是框的匹配策略     
预测框绑定到与其 IoU 最大的真实框上，真实框要与其 IoU >0.5 的预测框绑定；未发生绑定的即为负样本；             
1. 为什么说借鉴了 FasterRCNN 的 RPN 思路；    
anchor 机制：RPN 中特征图上一个点对应原图中的一组框，只不过这组框在 SSD 中更名为 prior box；                          
1. 一阶段和两阶段的区别？   
两阶段：先进行 RPN（对 box 粗回归 + 前背景分类），筛选过后，用 ROIPooling 将目标对应的特征图裁剪出来，裁剪结果依次进行二次回归 + 多分类；    
单阶段一步到位，直接进行多分类 + box 回归；  
1. 框是怎么映射回去的    
不需要映射，anchor 本身就有宽高，回归的结果是比例，直接修正就可以；也就是说，预测值本身就是针对原始图片做的；    
1. 为什么不用 IoU 做损失    
可以用，而且抗尺度影响；    

*派生算法有：DSSD（上采样特征融合）、RSSD（彩虹特征融合）、RefineDet（FasterRCNN 结合 SSD）、RFBNet（多个感受野融合）*     

<span id='DSSD'>  </span>

**DSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；    
```
backbone： SSD + ResNet101     
neck： 特征融合（反卷积+相乘）     
head： 使用了残差结构    
```  
结果：效率降低了 7 倍，精度就提升了 1 个点不到；  

1. 深层反卷积，浅层叠了两层标配卷积，然后再相乘的；*加了好多参数量,加法融合后 mAP 提高了 1.3，乘法再提高 0.2*     
1. 预测时的残差结构令 mAP 提升了 0.5 ～ 0.7；   

*特征融合包括：逐元素相加（FPN）、逐元素相乘（DSSD）、通道拼接*    
>
这么多的参数量，才带来 2 个百分点的提升，工业中很不值得     
对于 VOC 数据集只提升了 2 个点，但不知道实际应用中和 SSD 的差距会有多少；    

<span id='DSSD'>  </span>

**RSSD**
{:.warning}
使用特征金字塔缓解小目标检测问题；并使用彩虹连接缓解误检问题；    
```
backbone： SSD + ResNet101     
neck： 彩虹融合；     
head： 用同一个分类网络（所有框共用一套分类参数），更稳定，收敛更快；
```    

**结果**：300 的输入时，效率比 SSD 降了一半，精度也没见提高啊；512 的输入倒是提高了 1 个点；单独的小目标召回确实高了2～4个点；  

**彩虹融合**：通常下采样用来将低层特征逐层堆叠到高层去，上采样则是将高层特征逐层堆叠到低层；此处就用 pooling 和反卷积同时将上下层特征图拼接到当前层，那么所有特征图的通道数就一样了；   
特征融合前用了 BN，达到归一化感受野和不同尺度的效果`怎么就达到这个效果了，确定不是为了强行使用 BN 而杜撰的概念`{:.warning}

**issue**   
1. 最后怎么做的分类，确定不是参数多了之后引起的分类更准确？     
其实 SSD 中最后分类时，需要针对每一张特征图定义一个分类层，反向传播时自然也是单独训练；因为这些分类层不改变特征图尺寸，只改变通道数，姑且认为等于分类数量，也就意味着最终经过分类层/预测层处理后的特征图通道数是一样的；而这里，因为彩虹融合使得每个用于预测的特征图通道相同，那么在输入输出通道数一样的情况下，及可以将多个分类层可以合并成一个；
>且不说效果怎么样，至少参数量是又降低了的；    

1. 和 DSSD 相比，同样使用了 ResNet101，为啥这个速度快了很多    
难道只是因为用 concate 代替了乘法，速度就提升了 4 倍？    

<span id='RefineDet'>  </span>

**RefineDet**
{:.warning}
提升检测精度       
```
backbone： SSD + RPN     
neck： FPN（反卷积 + 相加）     
```    
SSD 搭配 RPN 先过滤掉大部分的负样本框；然后将剩下的框直接送入后续的预测——FPN + SSD；     
*借鉴了 FasterRCNN 的 RPN，但是没有 ROIPooling 层，就算不得是两阶段；*     
>虽然是在融合 FasterRCNN，但是没有在融合他最影响精度的点；     


<span id='RFBNet'>  </span>

**RFBNet**
{:.warning}
提升检测精度：用 RFB 替代 SSD 的头部；           
```
backbone： VGG   
head： RFB
```    
**RFB**：head 部分用了类似 Inception 的 block 代替单一感受野；block 内用了多种尺度的卷积核和空洞卷积；     
速度稍有下降的情况下，精度提升了 3%；*DSSD、RefineDet 等速度急剧下降才换取少量精度提升；*    


<span id='YOLOv1'>  </span>

**YOLOv1**
{:.warning}
提升速度       
```
backbone： darknet24（神似 GoogLeNet，448×448，比 VGG 快）     
```    
**基准框**：将图片划分成 s×s 个网格，每个网格负责预测 B 个目标和 C 个类别；     
此处 B 为 2；s 为 7，最终由特征图大小控制，即特征图上一个点对应原图一个区域，而且相邻区域无重叠，这不就是滑窗策略吗；`但如果是滑窗，就应该用全卷积，而最后用了几层全连接打乱了位置信息，所以这个区域映射的思路逻辑上是不通的`{:.warning}    
**预测输出**：$B \times box + C$，其中 $box$ 为 5（x, y, w, h, confidence）；也就是说特征图上每个点预测 2 个框，这两个框有不同的位置信息，但是共用一个类别信息，最终会舍弃 confidence 低的那个物体；*此 condifence 预测的是 box 的 IoU*    
box 回归时，default box 只有一个信息就是中心坐标；回归结果中包含了中心的偏移量和宽高；那么其实可以理解为 anchor 是一组只有中心坐标、宽高为 0 的框，后续预测仍旧使用了框的概念，所以很多文献把 YOLO 归类为 anchor free 是不合理的；   
**预测框与真值框绑定**：两种框中心点落在同一格子的即绑定，和 IoU 绑定策略不同；     
FastYolo：darknet19，达到 155fps；    

**缺点**：    
1. 定位误差大`归咎于下采样率高`{:.warning}，小目标效果差、召回率低（*损失函数没着重考虑小目标，下采样率高，没有 FPN*）；   
1. 邻近目标（密集）易漏检（因为一个格子只预测两个同类别目标），比如说人群、鸟群；     
1. 非正方形目标效果略差；       

**Issue**:    
1. 怎么控制被检测物体的尺度在一个范围的，多大的范围？    
宽高回归在（0, 1），基准是图片的宽高，这样最大的目标宽高就有界限了；中心坐标也一样，以下采样尺度，即 s×s 格子的大小为限`应该是`{:.warning}     

>关于 box IoU confidence 的思路很先进啊，这个想法很多网络都没考虑，是到后来的一些检测网络才考虑到这个点可以提升 NMS 的效果；    


*为什么感觉简单的逻辑被作者说得都迷糊了*    

<span id='YOLOv2'>  </span>

**YOLOv2**
{:.warning}
提升精度       
```
backbone： darknet19      
```    
**darknet19**：    
- 输入416×416，32倍下采样；精度堪比 VGG，运算量是 1/4；    
- 3×3 代替 7×7；    
- BN 提高了 2 个百分点；   
- 去除了 fc 和 dropout；   
- 特征融合：pooling + concate，提升了 1%；`这是哪门子融合啊，别人都是把高层语义融合到下层，这个 pooling 上去有啥意义`{:.warning}     

**基准框**：使用 anchor 机制，一组 5 个；尺度也是从训练集聚类所得，选平均 IoU 最大的；    
**预测输出**：预测用 1×1 的卷积在通道维度上做，很合逻辑啊；$(class + 1 + box) \times 5$，box 为 4，丢掉了 IoU 置信度预测；    
**预测框与真实框绑定**：中心落在同一区域的就绑定；最大 IoU 的为正样本，IoU < 0.6 的为负；`奇怪`{:.warning}     
**工程技巧**：    
- 多尺度输入，没有用 fc，可以接受任意尺寸输入；*这样不用重新训练就可以随意选择精度为主还是速度为主*    
- crop + resize 实现单尺度输入下的多尺度目标；    

**缺点**：    
- 单层预测，细粒度效果一般；    
- 小物体一般；    
- 工程技巧多，导致扩展性不够好；    

**Issue**：    
1. YOLO9000 怎么预测未知类别的？    
YOLO9000 使用的是层次 softmax；所谓的未知类别，说的是物体本身的类别，而可预测的是该类别的父类别；也就是用父类别来保底；    

1. 为什么取消掉 IoU 的预测呢，不好用吗？    
1. 为什么说预测框偏移量比直接预测更容易，不就是差了个基准线吗？


<span id='YOLOv3'>  </span>

**YOLOv3**
{:.warning}
提升精度       
```
backbone： darknet53（416×416）     
neck： FPN（uppooling + concate）     
head： 多层
anchor： 聚类得到一组 9 个，每层分配三个
cls： softmax 换成了 logistic 回归（解决多标签问题）
```    
**darknet53**：     
融入了残差结构；速度直接下降；    
tiny darknet：速度和模型大小都是最佳；     
**预测输出**： $(class + 1 + box) \times 3$，`那个 1 代表背景和代表 IoU 有什么区别`{:.warning}     
**优点**：背景误检低（正样本生成严格）`什么意思`{:.warning}     
**缺点**：定位误差大，召回低，拥挤效果差；`不是都使用 anchor 了吗`{:.warning}     
业界应用多，相同精度下，推理速度更高；     

*focalloss 有被尝试过，但是没有生效，原因不详，或是因为那个前背景分类；*

<span id='YOLOv4'>  </span>

**YOLOv4**
{:.warning}
提升精度       
技巧的堆叠，框架过于复杂了；    
**CSP**：    
- GPU：ResNeXt50，DarkNet53     
- VPU：EfficientNet-lite，MixNet，GhostNet，MobileNetV3；     

**Neck**：    
*FPN，SFAM，ASFF，BiFPN*     
- FPN：两层相加；    
- GFM：N层相加；    
- EFM：加两次，先上中下，再上中加；    

**数据增强**：颜色抖动，噪声，缩放，crip，flip，rotate，erase，mixup（全图融合），黑边扰动，cutmix（切块贴上去），cutout，mosaic（多张图拼在一起）；   
**回归**：GIoU，DIoU，CIoU；    
**增大感受野**：SPP，ASPP，RFB；    
**注意力**：SE，SpatialAM；    
**NMS**：SoftNMS，DIoUNMS；    

<span id='RetinaNet'>  </span>

**RetinaNet**
{:.warning}
样本不均衡       
```
backbone： ResNet     
neck： FPN
head：多层预测
cls_loss：focalloss
```    
FPN 增加小目标检测精度；    
focal loss 解决样本不均衡问题；    
二分类交叉熵：&emsp;&emsp;&emsp;&emsp;$$
CE(p,y)=\left\{
\begin{aligned}
&-\log (p) & if \quad y = 1 \\
&-\log (1-p) & otherwise
\end{aligned}
\right.
$$    
为了简化定义，我们定义：&emsp;&emsp;&ensp;$$
p_t=\left\{
\begin{aligned}
&p & if \quad y = 1 \\
&1-p & otherwise
\end{aligned}
\right.
$$    
则：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$CE(p,y) = CE(p_t) = -\log(p_t)$$    

平衡交叉熵：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$CE(p_t) = -\alpha_t\log(p_t)$$   
为了处理类别间不均衡的情况，我们为每一个类别赋值一个权重 $\alpha, \quad \alpha \in (0, 1), \sum_t\alpha_t = 1$；     
focal loss：&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$$FL(p_t) = -(1-p_t)^\gamma\log(p_t)$$

issue:    
1. 上采样怎么实现的，最近邻和cnn？    

### c free anchor

## B 参考资料
1. [目标检测比赛中的tricks](https://zhuanlan.zhihu.com/p/102817180)
1. [目标检测算法中检测框合并策略技术综述](https://zhuanlan.zhihu.com/p/48169867)     
1. [目标检测比赛提高mAP的方法](https://www.cnblogs.com/zi-wang/p/12537034.html)    
1. [目标检测入门，看这篇就够了](https://zhuanlan.zhihu.com/p/34142321)-glint     
1. [Real-Time Object Detection on COCO](https://paperswithcode.com/sota/real-time-object-detection-on-coco)-帧率对比    
